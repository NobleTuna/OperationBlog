---
title: 빅데이터 도메인 공부
categories:
  - Development
  - BigData
tags:
  - bigdata
  - clustering
date: 2019-06-25 07:09:31
---


![2019-03-30 19:26](/image/apartmentCloud.JPG)

수업내용을 내가 이해한대로 받아적은것이라 난잡하다
어설프게 한글사이에 영어가 섞여있는건 강사님의 말을 내 뇌피유가 따라가지 못했기 때문
관련 프로젝트 진행하기 전에 한번 훑으면 아이디어 등에 도움될듯 해서 남긴다
<hr>

### 클러스터링(Clustering)
데이터를 유사도에 의해서 K개의 그룹으로 나누는것
추천 시스템에서 주로 사용

- 백화점 고객을 구매 상품에 따라서 클러스터링함
- 추천 시스템을 의해 고객의 과거 패턴을 이용해서 클러스터링 함
- Gene 데이터를 유사도에 따라서 클러스터링 함
- 텍스트 문서들을 주제에 따라서 클러스터링 함
- Facebook 에서 이미지들을 유사한 이미지들로 클러스터링을 함
- Call center에서 고객과 통화한 내용을 텍스트로 변환 하여 그 내용에 나타나는 단어나 어휘구나 어휘 절을 추출하고 이를 이용하여 각각의 통화내역을 그룹으로 나누어서 회사에 걸러온 상담 내용을 카테고리 별로 나누어 보고 각 카테고리 별로 요약 정보를 만들어냄

클러스터링을 다 비교해가면서 하기엔 시간이 많이 소모되므로 대부분 근사치를 이용한 클러스터링을 사용한다.
<hr>

### K-Means Clustering
평균점을 계산하여 센터를 계속 찾은 후 (무게중심) 각 센터에 가까운 것들끼리 클러스터링, 반복
클러스터링 변화가 없으면 멈춤
처음에 랜덤하게 시작하는 파티션에 따라서 결과값이 다르기 때문에 여러번 시도하고 그중에 가장 좋은 것으로 결정

**단점**
* 클러스터링 사이즈가 잘 맞지 않음(?)
* 클러스터가 공 모양으로 나온다
* outliers가 있을 경우 평균점 계산때문에 부정확한 결과가 나옴 -> K-Medoids 알고리즘

간단하지만 실제로 사용하지 않음
<hr>

### Hierarchical Clustering (계층 구조)
top-down(하향식), bottom-up(상향식) 방식 두가지가 있음
보통 bottom-up 방식 사용

토너먼트 대진표처럼 가까운것끼리 묶어나감
가장 가까운 클러스터 두개를 merge, 클러스터가 k개가 될때까지 반복
어떤 distance function을 쓰냐에 따라서 다름

**distance function(각 클러스터간의 거리를 무엇을 기준으로 비교할 것인가)**
- single-link : 각각 클러스터링 안에 제일 가까운 데이터 거리를 비교하고 머지
- complete-link : 각각 클러스터링 안에서 제일 먼 데이터를 비교하고 머지, 클러스터 크기는 작게나옴
- average-link : 각 클러스터의 데이터끼리 1:1로 거리를 비교하고 평균을냄
- mean-link : 섞어놓고 전체 모든 점들간 거리 평균을 계산함
- controid-link  : 클러스터링의 센터값과 거리가 제일 가까운것과 머지,센터는 보통 평균을 씀, 여러개이면 랜덤한것을 클러스터링, 중심은 그것들의 센터값으로 비교

**단점**
- centroid-link 를 사용하면 k-means와 비슷하게됨 (동그랗게)
- single link를 사용하면 클러스터가 길쭉하게 나옴
- 디스턴스 펑션에 따라 아웃라이어를 포함하는 경우도 생김
- 속도가 느림(데이터 갯수에 세제곱에 비례함)

**장점**
- k-means와 다르게 최대 거리조건을 주면 클러스터 개수를 지정하지 않아도 (카테고리 갯수를 몰라도 ) 결과를 알 수 있음

<hr>

### DBSCAN Clustering
점 하나에서 같은 반지름으로 묶어지는놈들을 계속해서 merge
unvisited 한놈을 계속해서 진행하면서 merge. 약간 2차원 BFS같은 느낌
반지름과 (eps) 최소 몇개이상 모여야 클러스터로 인정할 것인가에 따라서 결과가 다름
<hr>

### EM Algorithm Clustering
모수에 대한 입력값(초기값)이 여러개가 나올것이고 그중에 확률적으로 제일 높은것을 선택
정규분포(클러스터)를 수정해가면서 정확도를 올림
생성모델 사용
생성모델(Generative Model) :  레이블이 없는 데이터도 학습하여 분류하는 모델

likelihood : 가능도. 확률
likelihood 가 제일 높은것으로 선택
결과값을 가지고 입력값을 추측하는 느낌
클러스터링마다 각 점마다 각 클러스터에 들어갈 확률을 수치화함
(그만큼 다른것들보다 자원소모(메모리)같은게 더 요구됨)
<hr>

### PLSI
생성모델 사용
몇개의 주제에 대해 클러스터링 해라
-> 각각의 단어마다 주제를 선택하고 주제에 대해 단어집에서 나올 확률을 보고 확률이 높은 단어 선정

(주제를 뽑을 확률) * (단어를 뽑을 확률)

우리가 이런식으로 글을 쓴다는 모델
모든 단어를 카운트하면 너무 커지니까 단어수에 제한을 두어 벡터화함
EM Clustering과 마찬가지로 각 단어가 어떤 주제에 해당하는지 알 수 있음
<hr>

### Content based filtering method
item 이나 product등과 같은 actual content를 이용함
각 아이템간의 연관도를 이용해서 추천함
<hr>

### Collaborative filtering method
각각의 유저는 비슷한 다른 유저와 동일하게 행동한다는 가정을 한 메소드
누군가의 추천이 다른 유저들의 추천에 영향을 끼침
User 가 직접 점수를 매긴 item들에 대한 rating 을 이용해서 추천
다른 유저들의 의견을 이용함

유저의 평가, 샀다 안샀다 등의 유저 정보로
유저의 평가를 보고 비슷한 평가군의 추천을 해줌
memory based method : 평가내역등을 가지고 비슷하게 평가
model based method : 과거의 레이팅으로 데이터로 구체적인 벡터(모델)을 만들어서 평가 (PLSI), matrix factorization
<hr>

### Matrix Factorization(행렬 분해)
곱해서 평점 매트릭스가 나오는 k개의 세로 유저행과 k개의 가로 영화수의 행렬을 구하고 곱해서 만든 행렬로 빈칸(평점을 매기지 않은 영화)를 추측할 수 있다.

학습을 통해 벡터를 내적했을때, 결과가 현재 가지고 있는 평점 데이터와 비슷한 벡터 두개(유저, 아이템)를 구함

곱하는 행렬이 여러개가 나올 수 있으니 정확하지 않을 수 있음
-> summary등 다른 텍스트 정보를 이용해 각각 벡터에 PLSI를 사용해 주제확률로 범위를 좁혀 좀더 정확한 행렬을 찾을 수 있음

단, 텍스트 정보가 연관성이 없을 경우 더 부정확한 결과가 나올 수 있으므로 PLSI를 먼저 확인하여 데이터 연관성, 노이즈 정도를 확인하고 사용해야한다
